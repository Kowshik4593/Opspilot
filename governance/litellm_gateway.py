# governance/litellm_gateway.py
"""
Enhanced LiteLLM Gateway
========================
Production-grade LLM gateway with:
- Response caching (reduces costs by 90% on repeated queries)
- Automatic retry with exponential backoff
- Streaming support for real-time responses
- Token budget enforcement
- Request/response logging
- Error handling and fallbacks

Optimized for single model (gpt-4o-mini) with smart prompt engineering
to maximize quality despite model constraints.
"""

from __future__ import annotations
from typing import Optional, Dict, Any, List, Generator, Union
from datetime import datetime, timedelta
import json
import time
import hashlib
from pathlib import Path
from dataclasses import dataclass, field
from openai import OpenAI
from config.settings import SETTINGS
from governance.usage import write_usage


# ============================================================
# CACHING SYSTEM
# ============================================================

@dataclass
class CacheEntry:
    """Cached LLM response"""
    prompt_hash: str
    response: str
    timestamp: str
    model: str
    tokens_in: int
    tokens_out: int
    metadata: Dict[str, Any] = field(default_factory=dict)


class DiskCache:
    """Simple disk-based cache for LLM responses"""
    
    def __init__(self, cache_dir: str = "./.llm_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.ttl_hours = 24  # Cache expires after 24 hours
    
    def _hash_prompt(self, prompt: str, model: str, temperature: float) -> str:
        """Create hash of prompt + params for cache key"""
        key = f"{prompt}|{model}|{temperature}"
        return hashlib.sha256(key.encode()).hexdigest()[:16]
    
    def get(self, prompt: str, model: str, temperature: float) -> Optional[str]:
        """Retrieve cached response if exists and not expired"""
        cache_key = self._hash_prompt(prompt, model, temperature)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        if not cache_file.exists():
            return None
        
        try:
            data = json.loads(cache_file.read_text(encoding="utf-8"))
            
            # Check expiry
            cached_time = datetime.fromisoformat(data["timestamp"])
            age = datetime.utcnow() - cached_time
            
            if age > timedelta(hours=self.ttl_hours):
                cache_file.unlink()  # Delete expired cache
                return None
            
            return data["response"]
        except Exception:
            return None
    
    def set(self, prompt: str, model: str, temperature: float, 
            response: str, tokens_in: int, tokens_out: int, metadata: Dict = None):
        """Store response in cache"""
        cache_key = self._hash_prompt(prompt, model, temperature)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        entry = CacheEntry(
            prompt_hash=cache_key,
            response=response,
            timestamp=datetime.utcnow().isoformat(),
            model=model,
            tokens_in=tokens_in,
            tokens_out=tokens_out,
            metadata=metadata or {}
        )
        
        cache_file.write_text(json.dumps(entry.__dict__, indent=2), encoding="utf-8")
    
    def clear(self):
        """Clear all cached entries"""
        for cache_file in self.cache_dir.glob("*.json"):
            cache_file.unlink()


# ============================================================
# PROMPT OPTIMIZATION
# ============================================================

class PromptOptimizer:
    """
    Optimize prompts for single model (gpt-4o-mini)
    Adds techniques to improve quality without multiple models:
    - Chain-of-thought prompting
    - Role-specific system messages
    - Output format constraints
    """
    
    ROLE_SYSTEM_MESSAGES = {
        "email_agent": """You are an expert email analyst and communication specialist.
Your role: Analyze emails, extract action items, draft professional responses.
Approach: Be concise, actionable, and context-aware.
Output: Always provide structured, JSON-compatible responses when requested.""",
        
        "meeting_agent": """You are a meeting intelligence specialist.
Your role: Analyze meeting transcripts, extract decisions, identify action items.
Approach: Be specific - use actual names, dates, and technical details from transcripts.
Output: Provide detailed, structured summaries with clear ownership.""",
        
        "tasks_agent": """You are a productivity and task management expert.
Your role: Analyze workload, prioritize tasks, create focus plans.
Approach: Consider urgency, dependencies, and user capacity.
Output: Provide actionable daily plans with time blocks.""",
        
        "wellness_agent": """You are a workplace wellness and burnout prevention specialist.
Your role: Monitor workload, detect stress signals, suggest interventions.
Approach: Be empathetic yet data-driven, focus on sustainable productivity.
Output: Provide wellness scores with actionable recommendations.""",
        
        "chat_agent": """You are an intelligent workplace assistant.
Your role: Help users with tasks, emails, meetings, and wellness through conversation.
Approach: Be conversational but efficient, ask clarifying questions when needed.
Output: Provide helpful, contextual responses based on user's data.""",
    }
    
    @classmethod
    def enhance_prompt(cls, agent_name: str, prompt: str, 
                       output_format: Optional[str] = None) -> tuple[str, str]:
        """
        Enhance prompt with CoT and format instructions
        Returns: (system_message, enhanced_prompt)
        """
        system_msg = cls.ROLE_SYSTEM_MESSAGES.get(
            agent_name,
            f"You are {agent_name}, a helpful AI assistant for workplace automation."
        )
        
        enhanced = prompt
        
        # Add Chain-of-Thought for complex reasoning
        if any(keyword in prompt.lower() for keyword in ["analyze", "extract", "decide", "plan"]):
            enhanced = f"""Think step by step:
1. First, understand the context and requirements
2. Then, identify key information
3. Finally, provide your response

Task:
{prompt}"""
        
        # Add output format constraints
        if output_format:
            enhanced += f"\n\nIMPORTANT: Return ONLY valid {output_format}. No markdown, no code fences, no extra text."
        
        return system_msg, enhanced


# ============================================================
# ENHANCED LITELLM GATEWAY
# ============================================================

class EnhancedLiteLLMGateway:
    """
    Production-grade LiteLLM gateway optimized for single model deployment
    
    Features:
    - Smart caching to reduce costs
    - Retry logic with exponential backoff
    - Streaming for real-time UX
    - Budget enforcement
    - Comprehensive error handling
    """
    
    def __init__(
        self,
        agent_name: str,
        daily_budget_usd: float = None,
        enable_cache: bool = True,
        enable_retry: bool = True
    ):
        self.agent_name = agent_name
        self.daily_budget = daily_budget_usd or SETTINGS["governance"]["daily_budget_usd"]
        self.model = SETTINGS["models"]["chat_model"]
        self.base_url = SETTINGS["models"]["azure_api_base"]
        self.api_key = SETTINGS["models"]["azure_api_key"]
        
        self.cache = DiskCache() if enable_cache else None
        self.enable_retry = enable_retry
        self.max_retries = 3
        
        self.optimizer = PromptOptimizer()
        
        # Initialize OpenAI client for LiteLLM proxy
        if self.api_key:
            self.client = OpenAI(
                base_url=self.base_url,
                api_key=self.api_key
            )
        else:
            self.client = None
    
    def call(
        self,
        prompt: str,
        temperature: float = 0.2,
        max_tokens: int = 1024,
        stream: bool = False,
        use_cache: bool = True,
        output_format: Optional[str] = None,  # "JSON" | "text"
        correlation_id: Optional[str] = None,
        **kwargs
    ) -> Union[str, Generator[str, None, None]]:
        """
        Make LLM call with optimizations
        
        Args:
            prompt: The prompt to send
            temperature: Sampling temperature (0.0 = deterministic)
            max_tokens: Maximum tokens in response
            stream: Whether to stream response
            use_cache: Whether to use cached responses
            output_format: Expected output format for validation
            correlation_id: For tracking related operations
        
        Returns:
            String response or generator if streaming
        """
        start_time = time.time()
        
        # Check cache first (only for non-streaming)
        if use_cache and not stream and self.cache:
            cached = self.cache.get(prompt, self.model, temperature)
            if cached:
                # Log cache hit
                write_usage(
                    self.agent_name,
                    self.model,
                    0, 0, 0, 0.0,
                    "success",
                    correlation_id=correlation_id,
                    meta={"cache_hit": True}
                )
                return cached
        
        # Check budget
        if not self._check_budget():
            raise RuntimeError(f"Daily budget exceeded for {self.agent_name}")
        
        # Optimize prompt
        system_msg, enhanced_prompt = self.optimizer.enhance_prompt(
            self.agent_name,
            prompt,
            output_format
        )
        
        # Simulation mode (no API key)
        if not self.client:
            return self._simulate_response(enhanced_prompt, output_format)
        
        # Make real API call with retry
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": enhanced_prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=stream,
                    timeout=30.0
                )
                
                if stream:
                    return self._handle_streaming(response, correlation_id)
                else:
                    content = response.choices[0].message.content
                    
                    # Log usage
                    usage = response.usage
                    latency_ms = int((time.time() - start_time) * 1000)
                    self._log_usage(
                        usage.prompt_tokens,
                        usage.completion_tokens,
                        latency_ms,
                        correlation_id
                    )
                    
                    # Cache response
                    if use_cache and self.cache:
                        self.cache.set(
                            prompt, self.model, temperature,
                            content,
                            usage.prompt_tokens,
                            usage.completion_tokens,
                            {"correlation_id": correlation_id}
                        )
                    
                    return content
                    
            except Exception as e:
                if attempt == self.max_retries - 1:
                    # Last attempt failed
                    self._log_error(str(e), correlation_id)
                    raise
                
                # Exponential backoff
                wait_time = 2 ** attempt
                time.sleep(wait_time)
        
        raise RuntimeError("All retry attempts failed")
    
    def call_structured(
        self,
        prompt: str,
        schema: Dict[str, Any],
        temperature: float = 0.0,
        correlation_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Call LLM and parse JSON response
        
        Args:
            prompt: The prompt
            schema: Expected JSON schema (for documentation)
            temperature: Use 0.0 for deterministic structured output
        
        Returns:
            Parsed JSON object
        """
        schema_str = json.dumps(schema, indent=2)
        enhanced_prompt = f"""{prompt}

Return your response as valid JSON matching this schema:
{schema_str}

CRITICAL: Return ONLY the JSON object. No markdown, no code fences, no explanations."""
        
        response = self.call(
            prompt=enhanced_prompt,
            temperature=temperature,
            output_format="JSON",
            correlation_id=correlation_id
        )
        
        # Parse JSON with error handling
        try:
            # Clean up response (remove markdown if present)
            clean_response = response.strip()
            if clean_response.startswith("```"):
                # Extract JSON from markdown code block
                lines = clean_response.split("\n")
                clean_response = "\n".join(lines[1:-1]) if len(lines) > 2 else clean_response
            
            return json.loads(clean_response)
        except json.JSONDecodeError as e:
            # Fallback: Try to extract JSON from text
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except:
                    pass
            
            raise ValueError(f"Failed to parse JSON response: {e}\nResponse: {response[:200]}")
    
    def _handle_streaming(
        self,
        response,
        correlation_id: Optional[str]
    ) -> Generator[str, None, None]:
        """Handle streaming response"""
        full_content = []
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_content.append(content)
                yield content
        
        # Log usage after streaming completes
        self._log_usage(
            0, len("".join(full_content)) // 4,  # Rough token estimate
            0,
            correlation_id,
            meta={"streaming": True}
        )
    
    def _check_budget(self) -> bool:
        """Check if within daily budget"""
        # For Phase 1, always allow (budget tracking to be enhanced)
        return True
    
    def _log_usage(
        self,
        tokens_in: int,
        tokens_out: int,
        latency_ms: int,
        correlation_id: Optional[str],
        meta: Dict = None
    ):
        """Log token usage"""
        # Approximate cost (adjust based on actual pricing)
        cost_per_1k = 0.00015  # $0.15 per 1M tokens for GPT-4o-mini
        cost = (tokens_in + tokens_out) / 1000 * cost_per_1k
        
        write_usage(
            self.agent_name,
            self.model,
            tokens_in,
            tokens_out,
            latency_ms,
            cost,
            "success",
            correlation_id=correlation_id,
            meta=meta or {}
        )
    
    def _log_error(self, error: str, correlation_id: Optional[str]):
        """Log error"""
        write_usage(
            self.agent_name,
            self.model,
            0, 0, 0, 0.0,
            "fail",
            correlation_id=correlation_id,
            meta={"error": error}
        )
    
    def _simulate_response(self, prompt: str, output_format: Optional[str]) -> str:
        """Simulate response when no API key (for demo)"""
        # Return reasonable defaults based on agent
        if output_format == "JSON":
            if "email" in prompt.lower():
                return json.dumps({
                    "category": "actionable",
                    "priority": "P1",
                    "actions": ["Review and respond"],
                    "summary": "This email requires your attention."
                })
            elif "meeting" in prompt.lower():
                return json.dumps({
                    "summary": "Meeting discussion summary",
                    "decisions": ["Decision 1", "Decision 2"],
                    "action_items": ["Action 1", "Action 2"],
                    "risks": [],
                    "dependencies": []
                })
            else:
                return json.dumps({"result": "Simulated response"})
        else:
            return f"This is a simulated response for demo purposes. The {self.agent_name} would normally provide detailed analysis here."


# ============================================================
# CONVENIENCE FUNCTIONS
# ============================================================

def create_gateway(agent_name: str, **kwargs) -> EnhancedLiteLLMGateway:
    """Factory function to create gateway"""
    return EnhancedLiteLLMGateway(agent_name, **kwargs)


def clear_cache():
    """Clear all cached LLM responses"""
    cache = DiskCache()
    cache.clear()
    print("âœ… LLM cache cleared")
